{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c41f579a-535d-4a03-9330-34223efaf048",
   "metadata": {},
   "source": [
    "# Text Classification using torchtext and a Embedding-FNN model in Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cff9ef-1a22-44b3-a50a-2d142a90ca5f",
   "metadata": {},
   "source": [
    "## Installing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a68b06ef-6c89-4825-bfa2-b9a1a54dbe89",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (2.0.0)\n",
      "Requirement already satisfied: torchdata in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (0.6.0)\n",
      "Requirement already satisfied: typing-extensions in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch) (4.7.1)\n",
      "Requirement already satisfied: triton==2.0.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch) (2.0.0)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch) (11.10.3.66)\n",
      "Requirement already satisfied: filelock in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch) (3.12.2)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch) (11.4.0.1)\n",
      "Requirement already satisfied: networkx in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch) (2.14.3)\n",
      "Requirement already satisfied: jinja2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch) (11.7.101)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch) (11.7.91)\n",
      "Requirement already satisfied: sympy in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch) (11.7.4.91)\n",
      "Requirement already satisfied: wheel in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.40.0)\n",
      "Requirement already satisfied: setuptools in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (67.6.1)\n",
      "Requirement already satisfied: cmake in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from triton==2.0.0->torch) (3.27.1)\n",
      "Requirement already satisfied: lit in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from triton==2.0.0->torch) (16.0.6)\n",
      "Requirement already satisfied: urllib3>=1.25 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torchdata) (2.0.4)\n",
      "Requirement already satisfied: requests in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torchdata) (2.31.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests->torchdata) (2023.7.22)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests->torchdata) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests->torchdata) (3.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e45b321-dff7-4d56-aba1-32a93edb12ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -U portalocker>=2.0.0 wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4205b92d-4020-4228-9ce6-5488757098c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext.datasets import AG_NEWS\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torch import nn\n",
    "import wandb\n",
    "import time\n",
    "\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torchtext.data.functional import to_map_style_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f60104-b7f9-43df-9cde-73684256d022",
   "metadata": {},
   "source": [
    "### Download the dataset and visualize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1abcf14-3bdb-418e-bddd-161ec5f9760d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,\n",
       " \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the dataset from torchtext datasets\n",
    "train_iter = iter(AG_NEWS(split=\"train\"))\n",
    "\n",
    "#Show an example\n",
    "next(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "567377fa-5f4a-4949-98a5-1473af941931",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the class names in a dictionary to facilitate human evaluation\n",
    "ag_news_label = {1: \"World\", 2: \"Sports\", 3: \"Business\", 4: \"Sci/Tec\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60aa7ba3-288d-413d-ae34-acc6506a8d1b",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b898494-ec12-4d03-a0f3-5e1246c26d18",
   "metadata": {},
   "source": [
    "Here is an example for typical NLP data processing with tokenizer and vocabulary. The first step is to build a vocabulary with the raw training dataset. Here we use built in factory function build_vocab_from_iterator which accepts iterator that yield list or iterator of tokens. Users can also pass any special symbols to be added to the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc2e018d-06b4-44e8-ab21-265e37edf994",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[475, 21, 30, 5297]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the tokenizer for english text\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "# Tokenize the text\n",
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "# Create the vocabulary dictionary \n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n",
    "# Define the unknown token\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "# Test and show a tokenized text example\n",
    "vocab(['here', 'is', 'an', 'example'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563bc1fa-83c8-4a9e-918d-bc3088889121",
   "metadata": {},
   "source": [
    "Prepare the text processing pipeline with the tokenizer and vocabulary. The text and label pipelines will be used to process the raw data strings from the dataset iterators. The text pipeline converts a text string into a list of integers based on the lookup table defined in the vocabulary. The label pipeline converts the label into integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51ec8e3b-8493-47e3-975c-e67d06e39172",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[475, 21, 2, 30, 5297]\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "#Convert text string into vocabulary items\n",
    "text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "# Convert to the right label, starting from 0\n",
    "label_pipeline = lambda x: int(x) - 1\n",
    "# Test both pipelines\n",
    "print(text_pipeline('here is the an example'))\n",
    "print(label_pipeline('4'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756700b7-4343-4216-85c4-f1bc704e0ac4",
   "metadata": {},
   "source": [
    "### Generate data batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e60c11ea-f518-444b-8701-f9eb34dc5114",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Defie the collation function for a batch data\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list, offsets = [], [], [0]\n",
    "    for _label, _text in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        offsets.append(processed_text.size(0))\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text_list = torch.cat(text_list)\n",
    "    return label_list.to(device), text_list.to(device), offsets.to(device)\n",
    "\n",
    "\n",
    "# Create a Dataloader using the collator\n",
    "dataloader = DataLoader(\n",
    "    train_iter, batch_size=8, shuffle=False, collate_fn=collate_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31af5d8-856c-46b1-b3bd-5059c686ffc0",
   "metadata": {},
   "source": [
    "## Create the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd77465-4d03-4a5f-a1c3-8fde6b3a688d",
   "metadata": {},
   "source": [
    "The model is composed of the nn.EmbeddingBag layer plus a linear layer for the classification purpose. nn.EmbeddingBag with the default mode of “mean” computes the mean value of a “bag” of embeddings. Although the text entries here have different lengths, nn.EmbeddingBag module requires no padding here since the text lengths are saved in offsets.\n",
    "\n",
    "Additionally, since nn.EmbeddingBag accumulates the average across the embeddings on the fly, nn.EmbeddingBag can enhance the performance and memory efficiency to process a sequence of tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e834067-5e80-419d-a312-db85b0f2d518",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TextClassificationModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=False)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        return self.fc(embedded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84a00ba-13b6-4450-90e9-8f4ccc1b3a2c",
   "metadata": {},
   "source": [
    "## Initialize the training process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa837e7-5746-4510-b9cb-9725c964fba4",
   "metadata": {},
   "source": [
    "Connect to Weigths&biases to track training metrics and model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "404a5347-7613-44ab-b80a-fdee79ddf08a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  ········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/studio-lab-user/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "40e7fc74-5a59-4f50-8201-c50ff57cb99e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Download the train and test dataset\n",
    "train_iter, test_iter = AG_NEWS()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c4f6cc-21e5-40ca-bf6e-c46062b75cdd",
   "metadata": {},
   "source": [
    "Define some paramete4rs and hyperparameters for the training phase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e9947f8d-3733-47e5-a898-4c34862663b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set a dictionary with the configuration partameters    \n",
    "config = {\n",
    "    'num_class' : len(set([label for (label, text) in train_iter])),\n",
    "    'vocab_size' : len(vocab),\n",
    "    'validation_split' : 0.1,    \n",
    "    'embedding_size' : 64,\n",
    "    'log_interval' : 500,\n",
    "    # Hyperparameters\n",
    "    'epochs' : 8,  # epoch\n",
    "    'lr' : 5,  # learning rate\n",
    "    'batch_size' : 64,  # batch size for training\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ec821cc5-f945-45d2-9907-dfe415d9fd41",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_class': 4, 'vocab_size': 95810, 'validation_split': 0.1, 'embedding_size': 64, 'log_interval': 500, 'epochs': 8, 'lr': 5, 'batch_size': 64}\n"
     ]
    }
   ],
   "source": [
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8041d6c0-9135-43c5-9961-7a2f63bc9e57",
   "metadata": {},
   "source": [
    "## Define the train and evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cccaa011-df5c-4edf-9dd5-afce66ddffee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(dataloader):\n",
    "    model.train()\n",
    "    total_acc, total_count, running_loss = 0, 0, 0\n",
    "    log_interval = config.log_interval\n",
    "    start_time = time.time()\n",
    "    # For every batch\n",
    "    for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "        # Reset gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Get predictions\n",
    "        predicted_label = model(text, offsets)\n",
    "        # Calculate loss\n",
    "        loss = criterion(predicted_label, label)\n",
    "        # Run backward\n",
    "        loss.backward()\n",
    "        # Clip gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        # Apply optimizer\n",
    "        optimizer.step()\n",
    "        # Calculate accuracy metric\n",
    "        total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "        total_count += label.size(0)\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        # Register training metrics\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print(\n",
    "                \"| epoch {:3d} | {:5d}/{:5d} batches \"\n",
    "                \"| accuracy {:8.3f}\".format(\n",
    "                    epoch, idx, len(dataloader), total_acc / total_count\n",
    "                )\n",
    "            )\n",
    "            # Log to wandb\n",
    "            wandb.log({'epoch': epoch, 'step': idx, 'loss': running_loss / log_interval, \n",
    "                       'accuracy': total_acc / total_count})\n",
    "            \n",
    "            total_acc, total_count, running_loss = 0, 0, 0\n",
    "            start_time = time.time()\n",
    "\n",
    "\n",
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "            predicted_label = model(text, offsets)\n",
    "            loss = criterion(predicted_label, label)\n",
    "            total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    return total_acc / total_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86777779-d602-44b1-abca-f129f8554370",
   "metadata": {},
   "source": [
    "## Run the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e1172385-1004-4220-bc85-9d997c4c6981",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/studio-lab-user/sagemaker-studiolab-notebooks/wandb/run-20230815_112423-8bhcviun</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/edumunozsala/text_classification_demo/runs/8bhcviun' target=\"_blank\">hopeful-shadow-5</a></strong> to <a href='https://wandb.ai/edumunozsala/text_classification_demo' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/edumunozsala/text_classification_demo' target=\"_blank\">https://wandb.ai/edumunozsala/text_classification_demo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/edumunozsala/text_classification_demo/runs/8bhcviun' target=\"_blank\">https://wandb.ai/edumunozsala/text_classification_demo/runs/8bhcviun</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Init the W&B job to collect metrics\n",
    "wandb.init(project='text_classification_demo', config=config, save_code=True, job_type='training')\n",
    "config = wandb.config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c59a7814-f5f0-4a55-9909-845c1771f050",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   500/ 1688 batches | accuracy    0.699\n",
      "| epoch   1 |  1000/ 1688 batches | accuracy    0.859\n",
      "| epoch   1 |  1500/ 1688 batches | accuracy    0.875\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time: 33.35s | valid accuracy    0.892 \n",
      "-----------------------------------------------------------\n",
      "| epoch   2 |   500/ 1688 batches | accuracy    0.898\n",
      "| epoch   2 |  1000/ 1688 batches | accuracy    0.900\n",
      "| epoch   2 |  1500/ 1688 batches | accuracy    0.900\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time: 34.55s | valid accuracy    0.899 \n",
      "-----------------------------------------------------------\n",
      "| epoch   3 |   500/ 1688 batches | accuracy    0.912\n",
      "| epoch   3 |  1000/ 1688 batches | accuracy    0.911\n",
      "| epoch   3 |  1500/ 1688 batches | accuracy    0.916\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | time: 30.89s | valid accuracy    0.908 \n",
      "-----------------------------------------------------------\n",
      "| epoch   4 |   500/ 1688 batches | accuracy    0.924\n",
      "| epoch   4 |  1000/ 1688 batches | accuracy    0.925\n",
      "| epoch   4 |  1500/ 1688 batches | accuracy    0.920\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   4 | time: 33.12s | valid accuracy    0.909 \n",
      "-----------------------------------------------------------\n",
      "| epoch   5 |   500/ 1688 batches | accuracy    0.930\n",
      "| epoch   5 |  1000/ 1688 batches | accuracy    0.928\n",
      "| epoch   5 |  1500/ 1688 batches | accuracy    0.930\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   5 | time: 31.27s | valid accuracy    0.906 \n",
      "-----------------------------------------------------------\n",
      "| epoch   6 |   500/ 1688 batches | accuracy    0.942\n",
      "| epoch   6 |  1000/ 1688 batches | accuracy    0.941\n",
      "| epoch   6 |  1500/ 1688 batches | accuracy    0.944\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   6 | time: 31.51s | valid accuracy    0.914 \n",
      "-----------------------------------------------------------\n",
      "| epoch   7 |   500/ 1688 batches | accuracy    0.943\n",
      "| epoch   7 |  1000/ 1688 batches | accuracy    0.945\n",
      "| epoch   7 |  1500/ 1688 batches | accuracy    0.943\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   7 | time: 31.74s | valid accuracy    0.913 \n",
      "-----------------------------------------------------------\n",
      "| epoch   8 |   500/ 1688 batches | accuracy    0.946\n",
      "| epoch   8 |  1000/ 1688 batches | accuracy    0.946\n",
      "| epoch   8 |  1500/ 1688 batches | accuracy    0.946\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   8 | time: 33.18s | valid accuracy    0.913 \n",
      "-----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Init the W&B job to collect metrics\n",
    "\n",
    "# define the model\n",
    "model = TextClassificationModel(config.vocab_size, config.embedding_size, config.num_class).to(device)\n",
    "\n",
    "# Set the loss function, the optimizer and the lr scheduler\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=config.lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "\n",
    "total_accu = None\n",
    "# Convert to Datasets\n",
    "train_dataset = to_map_style_dataset(train_iter)\n",
    "test_dataset = to_map_style_dataset(test_iter)\n",
    "# Split the training dataset into a train and validation datasets\n",
    "num_train = int(len(train_dataset) * (1.0-config.validation_split))\n",
    "split_train_, split_valid_ = random_split(\n",
    "    train_dataset, [num_train, len(train_dataset) - num_train]\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    split_train_, batch_size=config.batch_size, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "valid_dataloader = DataLoader(\n",
    "    split_valid_, batch_size=config.batch_size, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=config.batch_size, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "\n",
    "for epoch in range(1, config.epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(train_dataloader)\n",
    "    accu_val = evaluate(valid_dataloader)\n",
    "    if total_accu is not None and total_accu > accu_val:\n",
    "        scheduler.step()\n",
    "    else:\n",
    "        total_accu = accu_val\n",
    "    print(\"-\" * 59)\n",
    "    print(\n",
    "        \"| end of epoch {:3d} | time: {:5.2f}s | \"\n",
    "        \"valid accuracy {:8.3f} \".format(\n",
    "            epoch, time.time() - epoch_start_time, accu_val\n",
    "        )\n",
    "    )\n",
    "    print(\"-\" * 59)\n",
    "    wandb.log({'epoch': epoch, 'validation_acc': accu_val})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f6efc0-e988-438f-9d4a-45c28dc6ebfa",
   "metadata": {},
   "source": [
    "## Evaluate the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b6a368d9-cd24-4afa-8461-baf0d8a694d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the results of test dataset.\n",
      "test accuracy    0.908\n"
     ]
    }
   ],
   "source": [
    "print(\"Checking the results of test dataset.\")\n",
    "accu_test = evaluate(test_dataloader)\n",
    "print(\"test accuracy {:8.3f}\".format(accu_test))\n",
    "wandb.log({'test_acc': accu_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "32b4f100-5d84-40b6-8f38-0ae82beee506",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52091b4d26df44dfbd6e39f38a7904b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.010 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.120714…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁▆▆▇▇▇▇▇▇▇▇▇█▇██████████</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇████</td></tr><tr><td>loss</td><td>█▄▃▃▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>step</td><td>▁▅█▁▅█▁▅█▁▅█▁▅█▁▅█▁▅█▁▅█</td></tr><tr><td>test_acc</td><td>▁</td></tr><tr><td>validation_acc</td><td>▁▃▆▆▅███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.9455</td></tr><tr><td>epoch</td><td>8</td></tr><tr><td>loss</td><td>0.17061</td></tr><tr><td>step</td><td>1500</td></tr><tr><td>test_acc</td><td>0.90803</td></tr><tr><td>validation_acc</td><td>0.91325</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">hopeful-shadow-5</strong> at: <a href='https://wandb.ai/edumunozsala/text_classification_demo/runs/8bhcviun' target=\"_blank\">https://wandb.ai/edumunozsala/text_classification_demo/runs/8bhcviun</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230815_112423-8bhcviun/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e723acb-27c5-4f1e-86b0-5be713e41976",
   "metadata": {},
   "source": [
    "## Make a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ad5c1654-417b-40b7-b29f-b4d4b121878a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a Sports news\n"
     ]
    }
   ],
   "source": [
    "def predict(text, text_pipeline):\n",
    "    with torch.no_grad():\n",
    "        text = torch.tensor(text_pipeline(text))\n",
    "        output = model(text, torch.tensor([0]))\n",
    "        return output.argmax(1).item() + 1\n",
    "\n",
    "\n",
    "ex_text_str = \"MEMPHIS, Tenn. – Four days ago, Jon Rahm was \\\n",
    "    enduring the season’s worst weather conditions on Sunday at The \\\n",
    "    Open on his way to a closing 75 at Royal Portrush, which \\\n",
    "    considering the wind and the rain was a respectable showing. \\\n",
    "    Thursday’s first round at the WGC-FedEx St. Jude Invitational \\\n",
    "    was another story. With temperatures in the mid-80s and hardly any \\\n",
    "    wind, the Spaniard was 13 strokes better in a flawless round. \\\n",
    "    Thanks to his best putting performance on the PGA Tour, Rahm \\\n",
    "    finished with an 8-under 62 for a three-stroke lead, which \\\n",
    "    was even more impressive considering he’d never played the \\\n",
    "    front nine at TPC Southwind.\"\n",
    "\n",
    "model = model.to(\"cpu\")\n",
    "\n",
    "print(\"This is a %s news\" % ag_news_label[predict(ex_text_str, text_pipeline)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5a8cbf-9b0b-430e-8eea-2ab900d62694",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
