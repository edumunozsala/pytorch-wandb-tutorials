{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.7.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.6 MB 6.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numpy>=1.20\n",
      "  Downloading numpy-1.25.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 18.3 MB 69.7 MB/s eta 0:00:01��█████████    | 16.0 MB 69.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: importlib-resources>=3.2.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from matplotlib) (5.12.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from matplotlib) (2.8.2)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.42.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.5 MB 86.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from matplotlib) (23.0)\n",
      "Collecting pyparsing<3.1,>=2.3.1\n",
      "  Downloading pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
      "\u001b[K     |████████████████████████████████| 98 kB 14.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cycler>=0.10\n",
      "  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Collecting pillow>=6.2.0\n",
      "  Downloading Pillow-10.0.0-cp39-cp39-manylinux_2_28_x86_64.whl (3.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.4 MB 89.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.4.4-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 74.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting contourpy>=1.0.1\n",
      "  Downloading contourpy-1.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (300 kB)\n",
      "\u001b[K     |████████████████████████████████| 300 kB 72.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: zipp>=3.1.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.15.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Installing collected packages: numpy, pyparsing, pillow, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.1.0 cycler-0.11.0 fonttools-4.42.0 kiwisolver-1.4.4 matplotlib-3.7.2 numpy-1.25.2 pillow-10.0.0 pyparsing-3.0.9\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchdata==0.6.0\n",
      "  Using cached torchdata-0.6.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "Collecting torchtext==0.15.1\n",
      "  Downloading torchtext-0.15.1-cp39-cp39-manylinux1_x86_64.whl (2.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.0 MB 7.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting spacy\n",
      "  Downloading spacy-3.6.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.6 MB 15.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting urllib3>=1.25\n",
      "  Downloading urllib3-2.0.4-py3-none-any.whl (123 kB)\n",
      "\u001b[K     |████████████████████████████████| 123 kB 86.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting torch==2.0.0\n",
      "  Downloading torch-2.0.0-cp39-cp39-manylinux1_x86_64.whl (619.9 MB)\n",
      "\u001b[K     |█████████████████▌              | 339.6 MB 129.7 MB/s eta 0:00:03                     | 57.1 MB 76.7 MB/s eta 0:00:08██▍                           | 85.8 MB 76.7 MB/s eta 0:00:07ta 0:00:06��███▊                        | 150.4 MB 85.3 MB/s eta 0:00:062 MB/s eta 0:00:05| 281.8 MB 95.2 MB/s eta 0:00:04████████▉                 | 288.0 MB 95.2 MB/s eta 0:00:04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 619.9 MB 9.9 kB/s  eta 0:00:011████████████         | 446.1 MB 95.3 MB/s eta 0:00:02�█▉ | 597.6 MB 16.1 MB/s eta 0:00:02\n",
      "\u001b[?25hCollecting requests\n",
      "  Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "\u001b[K     |████████████████████████████████| 62 kB 1.2 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torchtext==0.15.1) (1.25.2)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "\u001b[K     |████████████████████████████████| 78 kB 5.7 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99\n",
      "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "\u001b[K     |████████████████████████████████| 849 kB 79.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch==2.0.0->torchdata==0.6.0) (4.5.0)\n",
      "Collecting nvidia-nccl-cu11==2.14.3\n",
      "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 177.1 MB 75 kB/s s eta 0:00:01██████████████████████▉       | 137.2 MB 55.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sympy\n",
      "  Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.7 MB 66.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu11==11.7.91\n",
      "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
      "\u001b[K     |████████████████████████████████| 98 kB 8.2 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.0.1\n",
      "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 102.6 MB 5.2 kB/s  eta 0:00:01�███████████████████▉   | 92.3 MB 84.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting triton==2.0.0\n",
      "  Downloading triton-2.0.0-1-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 63.3 MB 139 kB/s  eta 0:00:01      | 24.0 MB 77.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99\n",
      "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 21.0 MB 29.5 MB/s eta 0:00:0100:01\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.7.101\n",
      "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.8 MB 34.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting networkx\n",
      "  Downloading networkx-3.1-py3-none-any.whl (2.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.1 MB 25.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting nvidia-curand-cu11==10.2.10.91\n",
      "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 54.6 MB 225 kB/s  eta 0:00:01�████████▉           | 35.5 MB 40.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch==2.0.0->torchdata==0.6.0) (3.1.2)\n",
      "Collecting nvidia-cublas-cu11==11.10.3.66\n",
      "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "\u001b[K     |████████████████████████████▉   | 285.6 MB 79.2 MB/s eta 0:00:011�████████████▌           | 203.3 MB 113.0 MB/s eta 0:00:02     |█████████████████████████▎      | 250.6 MB 113.0 MB/s eta 0:00:01"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 317.1 MB 24 kB/s \n",
      "\u001b[?25hCollecting filelock\n",
      "  Downloading filelock-3.12.2-py3-none-any.whl (10 kB)\n",
      "Collecting nvidia-cufft-cu11==10.9.0.58\n",
      "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 168.4 MB 64 kB/s /s eta 0:00:01��██             | 100.0 MB 113.1 MB/s eta 0:00:01     |█████████████████████████████▉  | 157.2 MB 113.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.4.91\n",
      "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 173.2 MB 25 kB/s s eta 0:00:01               | 41.0 MB 63.0 MB/s eta 0:00:03     |██████████████▋                 | 79.2 MB 20.3 MB/s eta 0:00:05     |███████████████████▊            | 106.5 MB 20.3 MB/s eta 0:00:04\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96\n",
      "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 557.1 MB 7.6 kB/s  eta 0:00:01                      | 40.5 MB 18.3 MB/s eta 0:00:29  | 46.2 MB 18.3 MB/s eta 0:00:28██████████████▎           | 352.5 MB 71.1 MB/s eta 0:00:03�████████▊           | 359.9 MB 7.6 MB/s eta 0:00:27�████████▎      | 440.2 MB 95.7 MB/s eta 0:00:02��█████████████▎      | 440.4 MB 95.7 MB/s eta 0:00:02███████████████████▎      | 440.6 MB 95.7 MB/s eta 0:00:02��██▏     | 455.8 MB 95.7 MB/s eta 0:00:02��██▊     | 464.9 MB 95.7 MB/s eta 0:00:01��███████████████████     | 471.3 MB 95.7 MB/s eta 0:00:01��█████████████▍    | 476.4 MB 95.7 MB/s eta 0:00:019 MB 1.2 MB/s eta 0:00:25��███████████▏| 542.7 MB 60.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0->torchdata==0.6.0) (67.6.1)\n",
      "Requirement already satisfied: wheel in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0->torchdata==0.6.0) (0.40.0)\n",
      "Collecting lit\n",
      "  Downloading lit-16.0.6.tar.gz (153 kB)\n",
      "\u001b[K     |████████████████████████████████| 153 kB 103.0 MB/s eta 0:00:01\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting cmake\n",
      "  Downloading cmake-3.27.1-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (26.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 26.0 MB 49.2 MB/s eta 0:00:01B/s eta 0:00:01██████████████▎                | 12.5 MB 49.2 MB/s eta 0:00:01██████████████████▎          | 17.3 MB 49.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4\n",
      "  Downloading pydantic-2.1.1-py3-none-any.whl (370 kB)\n",
      "\u001b[K     |████████████████████████████████| 370 kB 26.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting spacy-legacy<3.1.0,>=3.0.11\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.9-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21 kB)\n",
      "Collecting smart-open<7.0.0,>=5.2.1\n",
      "  Downloading smart_open-6.3.0-py3-none-any.whl (56 kB)\n",
      "\u001b[K     |████████████████████████████████| 56 kB 7.1 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy) (23.0)\n",
      "Collecting thinc<8.2.0,>=8.1.8\n",
      "  Downloading thinc-8.1.12-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (935 kB)\n",
      "\u001b[K     |████████████████████████████████| 935 kB 71.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.7-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35 kB)\n",
      "Collecting typer<0.10.0,>=0.3.0\n",
      "  Downloading typer-0.9.0-py3-none-any.whl (45 kB)\n",
      "\u001b[K     |████████████████████████████████| 45 kB 5.1 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting langcodes<4.0.0,>=3.2.0\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "\u001b[K     |████████████████████████████████| 181 kB 80.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pathy>=0.10.0\n",
      "  Downloading pathy-0.10.2-py3-none-any.whl (48 kB)\n",
      "\u001b[K     |████████████████████████████████| 48 kB 9.6 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Downloading spacy_loggers-1.0.4-py3-none-any.whl (11 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1\n",
      "  Downloading wasabi-1.1.2-py3-none-any.whl (27 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3\n",
      "  Downloading srsly-2.4.7-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (492 kB)\n",
      "\u001b[K     |████████████████████████████████| 492 kB 52.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.9-py3-none-any.whl (17 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.8-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (128 kB)\n",
      "\u001b[K     |████████████████████████████████| 128 kB 42.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pydantic-core==2.4.0\n",
      "  Downloading pydantic_core-2.4.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.9 MB 50.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting typing-extensions\n",
      "  Downloading typing_extensions-4.7.1-py3-none-any.whl (33 kB)\n",
      "Collecting annotated-types>=0.4.0\n",
      "  Downloading annotated_types-0.5.0-py3-none-any.whl (11 kB)\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (202 kB)\n",
      "\u001b[K     |████████████████████████████████| 202 kB 88.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting certifi>=2017.4.17\n",
      "  Downloading certifi-2023.7.22-py3-none-any.whl (158 kB)\n",
      "\u001b[K     |████████████████████████████████| 158 kB 75.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: idna<4,>=2.5 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests->torchdata==0.6.0) (3.4)\n",
      "Collecting blis<0.8.0,>=0.7.8\n",
      "  Downloading blis-0.7.10-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.2 MB 53.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting confection<1.0.0,>=0.0.1\n",
      "  Downloading confection-0.1.1-py3-none-any.whl (34 kB)\n",
      "Collecting click<9.0.0,>=7.1.1\n",
      "  Downloading click-8.1.6-py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 10.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from jinja2->torch==2.0.0->torchdata==0.6.0) (2.1.2)\n",
      "Collecting mpmath>=0.19\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[K     |████████████████████████████████| 536 kB 95.2 MB/s eta 0:00:01�█████████████████       | 419 kB 95.2 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: lit\n",
      "  Building wheel for lit (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for lit: filename=lit-16.0.6-py3-none-any.whl size=93583 sha256=c676ace47c7c855eae39c80c3902aa982a082f676b117ee590cd6a5dc8ac9539\n",
      "  Stored in directory: /home/studio-lab-user/.cache/pip/wheels/a5/36/d6/cac2e6fb891889b33a548f2fddb8b4b7726399aaa2ed32b188\n",
      "Successfully built lit\n",
      "Installing collected packages: typing-extensions, pydantic-core, nvidia-cublas-cu11, mpmath, lit, filelock, cmake, catalogue, annotated-types, urllib3, triton, sympy, srsly, pydantic, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-cusolver-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cudnn-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, networkx, murmurhash, cymem, click, charset-normalizer, certifi, wasabi, typer, torch, smart-open, requests, preshed, confection, blis, tqdm, torchdata, thinc, spacy-loggers, spacy-legacy, pathy, langcodes, torchtext, spacy\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing-extensions 4.5.0\n",
      "    Uninstalling typing-extensions-4.5.0:\n",
      "      Successfully uninstalled typing-extensions-4.5.0\n",
      "Successfully installed annotated-types-0.5.0 blis-0.7.10 catalogue-2.0.9 certifi-2023.7.22 charset-normalizer-3.2.0 click-8.1.6 cmake-3.27.1 confection-0.1.1 cymem-2.0.7 filelock-3.12.2 langcodes-3.3.0 lit-16.0.6 mpmath-1.3.0 murmurhash-1.0.9 networkx-3.1 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 pathy-0.10.2 preshed-3.0.8 pydantic-2.1.1 pydantic-core-2.4.0 requests-2.31.0 smart-open-6.3.0 spacy-3.6.1 spacy-legacy-3.0.12 spacy-loggers-1.0.4 srsly-2.4.7 sympy-1.12 thinc-8.1.12 torch-2.0.0 torchdata-0.6.0 torchtext-0.15.1 tqdm-4.66.1 triton-2.0.0 typer-0.9.0 typing-extensions-4.7.1 urllib3-2.0.4 wasabi-1.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install torchdata==0.6.0 torchtext==0.15.1 spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For tips on running notebooks in Google Colab, see\n",
    "# https://pytorch.org/tutorials/beginner/colab\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Preprocess custom text dataset using Torchtext\n",
    "\n",
    "Extracted fromnotebooks by [Anupam Sharma](https://anp-scp.github.io/)\n",
    "\n",
    "This tutorial illustrates the usage of torchtext on a dataset that is not built-in. In the tutorial,\n",
    "we will preprocess a dataset that can be further utilized to train a sequence-to-sequence\n",
    "model for machine translation (something like, in this tutorial: [Sequence to Sequence Learning\n",
    "with Neural Networks](https://github.com/bentrevett/pytorch-seq2seq/blob/master/1%20-%20Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb)) but without using legacy version\n",
    "of torchtext.\n",
    "\n",
    "In this tutorial, we will learn how to:\n",
    "\n",
    "* Read a dataset\n",
    "* Tokenize sentence\n",
    "* Apply transforms to sentence\n",
    "* Perform bucket batching\n",
    "\n",
    "Let us assume that we need to prepare a dataset to train a model that can perform English to\n",
    "German translation. We will use a tab-delimited German - English sentence pairs provided by\n",
    "the [Tatoeba Project](https://tatoeba.org/en) which can be downloaded from\n",
    "[this link](https://www.manythings.org/anki/deu-eng.zip)_.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, download the dataset, extract the zip, and note the path to the file `deu.txt`.\n",
    "\n",
    "Here, we are using `Spacy` to tokenize text.\n",
    "\n",
    "Download the English and German models from Spacy as shown below:\n",
    "\n",
    "```shell\n",
    "python -m spacy download en_core_web_sm\n",
    "python -m spacy download de_core_news_sm\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.6.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.6.0/en_core_web_sm-3.6.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.8 MB 6.5 MB/s eta 0:00:01 MB 6.5 MB/s eta 0:00:02��███▊    | 11.1 MB 6.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from en-core-web-sm==3.6.0) (3.6.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (23.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.8)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.1.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (6.3.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.7)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.1.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.12)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.4.7)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.9.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.31.0)\n",
      "Requirement already satisfied: setuptools in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (67.6.1)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.12)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.9)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.10.2)\n",
      "Requirement already satisfied: jinja2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.1.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.25.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.9)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.7.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.5.0)\n",
      "Requirement already satisfied: pydantic-core==2.4.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2023.7.22)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.2.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.1.1)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.7.10)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.1.2)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.6.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "Collecting de-core-news-sm==3.6.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.6.0/de_core_news_sm-3.6.0-py3-none-any.whl (14.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 14.6 MB 6.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from de-core-news-sm==3.6.0) (3.6.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (4.66.1)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (1.1.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (2.1.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (1.0.9)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (0.9.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (0.10.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (1.25.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (2.0.9)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (2.4.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (3.0.8)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (3.3.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (2.0.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (23.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (6.3.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (1.0.4)\n",
      "Requirement already satisfied: jinja2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (67.6.1)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (8.1.12)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (3.0.12)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (2.31.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (0.5.0)\n",
      "Requirement already satisfied: pydantic-core==2.4.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (2023.7.22)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (0.1.1)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (0.7.10)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (8.1.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from jinja2->spacy<3.7.0,>=3.6.0->de-core-news-sm==3.6.0) (2.1.2)\n",
      "Installing collected packages: de-core-news-sm\n",
      "Successfully installed de-core-news-sm-3.6.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('de_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download de_core_news_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start by importing required modules:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torchdata.datapipes as dp\n",
    "import torchtext.transforms as T\n",
    "import spacy\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "eng = spacy.load(\"en_core_web_sm\") # Load the English model to tokenize English text\n",
    "de = spacy.load(\"de_core_news_sm\") # Load the German model to tokenize German text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will load the dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "FILE_PATH = 'data/deu.txt'\n",
    "data_pipe = dp.iter.IterableWrapper([FILE_PATH])\n",
    "data_pipe = dp.iter.FileOpener(data_pipe, mode='rb')\n",
    "data_pipe = data_pipe.parse_csv(skip_lines=0, delimiter='\\t', as_tuple=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code block, we are doing following things:\n",
    "\n",
    "1. At line 2, we are creating an iterable of filenames\n",
    "2. At line 3, we pass the iterable to `FileOpener` which then\n",
    "   opens the file in read mode\n",
    "3. At line 4, we call a function to parse the file, which\n",
    "   again returns an iterable of tuples representing each rows\n",
    "   of the tab-delimited file\n",
    "\n",
    "DataPipes can be thought of something like a dataset object, on which\n",
    "we can perform various operations.\n",
    "Check [this tutorial](https://pytorch.org/data/beta/dp_tutorial.html) for more details on\n",
    "DataPipes.\n",
    "\n",
    "We can verify if the iterable has the pair of sentences as shown\n",
    "below:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Go.', 'Geh.', 'CC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #8597805 (Roujin)')\n"
     ]
    }
   ],
   "source": [
    "for sample in data_pipe:\n",
    "    print(sample)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we also have attribution details along with pair of sentences. We will\n",
    "write a small function to remove the attribution details:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def removeAttribution(row):\n",
    "    \"\"\"\n",
    "    Function to keep the first two elements in a tuple\n",
    "    \"\"\"\n",
    "    return row[:2]\n",
    "\n",
    "data_pipe = data_pipe.map(removeAttribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `map` function at line 6 in above code block can be used to apply some function\n",
    "on each elements of `data_pipe`. Now, we can verify that the `data_pipe` only contains\n",
    "pair of sentences.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Go.', 'Geh.')\n"
     ]
    }
   ],
   "source": [
    "for sample in data_pipe:\n",
    "    print(sample)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us define few functions to perform tokenization:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def engTokenize(text):\n",
    "    \"\"\"\n",
    "    Tokenize an English text and return a list of tokens\n",
    "    \"\"\"\n",
    "    return [token.text for token in eng.tokenizer(text)]\n",
    "\n",
    "def deTokenize(text):\n",
    "    \"\"\"\n",
    "    Tokenize a German text and return a list of tokens\n",
    "    \"\"\"\n",
    "    return [token.text for token in de.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above function accepts a text and returns a list of words\n",
    "as shown below:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Have', 'a', 'good', 'day', '!', '!', '!']\n",
      "['Haben', 'Sie', 'einen', 'guten', 'Tag', '!', '!', '!']\n"
     ]
    }
   ],
   "source": [
    "print(engTokenize(\"Have a good day!!!\"))\n",
    "print(deTokenize(\"Haben Sie einen guten Tag!!!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the vocabulary\n",
    "Let us consider an English sentence as the source and a German sentence as the target.Vocabulary can be considered as the set of unique words we have in the dataset.We will build vocabulary for both our source and target now.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getTokens(data_iter, place):\n",
    "    \"\"\"\n",
    "    Function to yield tokens from an iterator. Since, our iterator contains\n",
    "    tuple of sentences (source and target), `place` parameters defines for which\n",
    "    index to return the tokens for. `place=0` for source and `place=1` for target\n",
    "    \"\"\"\n",
    "    for english, german in data_iter:\n",
    "        if place == 0:\n",
    "            yield engTokenize(english)\n",
    "        else:\n",
    "            yield deTokenize(german)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will build vocabulary for source:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "source_vocab = build_vocab_from_iterator(\n",
    "    getTokens(data_pipe,0),\n",
    "    min_freq=2,\n",
    "    specials= ['<pad>', '<sos>', '<eos>', '<unk>'],\n",
    "    special_first=True\n",
    ")\n",
    "source_vocab.set_default_index(source_vocab['<unk>'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above, builds the vocabulary from the iterator. In the above code block:\n",
    "\n",
    "* At line 2, we call the `getTokens()` function with `place=0` as we need vocabulary for\n",
    "  source sentences.\n",
    "* At line 3, we set `min_freq=2`. This means, the function will skip those words that occurs\n",
    "  less than 2 times.\n",
    "* At line 4, we specify some special tokens:\n",
    "\n",
    "  * `<sos>` for start of sentence\n",
    "  * `<eos>` for end of sentence\n",
    "  * `<unk>` for unknown words. An example of unknown word is the one skipped because of\n",
    "    `min_freq=2`.\n",
    "  * `<pad>` is the padding token. While training, a model we mostly train in batches. In a\n",
    "    batch, there can be sentences of different length. So, we pad the shorter sentences with\n",
    "    `<pad>` token to make length of all sequences in the batch equal.\n",
    "\n",
    "* At line 5, we set `special_first=True`. Which means `<pad>` will get index 0, `<sos>` index 1,\n",
    "  `<eos>` index 2, and <unk> will get index 3 in the vocabulary.\n",
    "* At line 7, we set default index as index of `<unk>`. That means if some word is not in\n",
    "  vocabulary, we will use `<unk>` instead of that unknown word.\n",
    "\n",
    "Similarly, we will build vocabulary for target sentences:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_vocab = build_vocab_from_iterator(\n",
    "    getTokens(data_pipe,1),\n",
    "    min_freq=2,\n",
    "    specials= ['<pad>', '<sos>', '<eos>', '<unk>'],\n",
    "    special_first=True\n",
    ")\n",
    "target_vocab.set_default_index(target_vocab['<unk>'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the example above shows how can we add special tokens to our vocabulary. The\n",
    "special tokens may change based on the requirements.\n",
    "\n",
    "Now, we can verify that special tokens are placed at the beginning and then other words.\n",
    "In the below code, `source_vocab.get_itos()` returns a list with tokens at index based on\n",
    "vocabulary.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad>', '<sos>', '<eos>', '<unk>', '.', 'I', 'Tom', 'to', 'you']\n"
     ]
    }
   ],
   "source": [
    "print(source_vocab.get_itos()[:9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numericalize sentences using vocabulary\n",
    "After building the vocabulary, we need to convert our sentences to corresponding indices.\n",
    "Let us define some functions for this:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getTransform(vocab):\n",
    "    \"\"\"\n",
    "    Create transforms based on given vocabulary. The returned transform is applied to sequence\n",
    "    of tokens.\n",
    "    \"\"\"\n",
    "    text_tranform = T.Sequential(\n",
    "        ## converts the sentences to indices based on given vocabulary\n",
    "        T.VocabTransform(vocab=vocab),\n",
    "        ## Add <sos> at beginning of each sentence. 1 because the index for <sos> in vocabulary is\n",
    "        # 1 as seen in previous section\n",
    "        T.AddToken(1, begin=True),\n",
    "        ## Add <eos> at beginning of each sentence. 2 because the index for <eos> in vocabulary is\n",
    "        # 2 as seen in previous section\n",
    "        T.AddToken(2, begin=False)\n",
    "    )\n",
    "    return text_tranform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us see how to use the above function. The function returns an object of `Transforms`\n",
    "which we will use on our sentence. Let us take a random sentence and check how the transform\n",
    "works.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some sentence=I giggled.\n",
      "Transformed sentence=[1, 5, 4894, 4, 2]\n",
      "<sos> I giggled . <eos> "
     ]
    }
   ],
   "source": [
    "temp_list = list(data_pipe)\n",
    "some_sentence = temp_list[798][0]\n",
    "print(\"Some sentence=\", end=\"\")\n",
    "print(some_sentence)\n",
    "transformed_sentence = getTransform(source_vocab)(engTokenize(some_sentence))\n",
    "print(\"Transformed sentence=\", end=\"\")\n",
    "print(transformed_sentence)\n",
    "index_to_string = source_vocab.get_itos()\n",
    "for index in transformed_sentence:\n",
    "    print(index_to_string[index], end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code,:\n",
    "\n",
    "* At line 2, we take a source sentence from list that we created from `data_pipe` at line 1\n",
    "* At line 5, we get a transform based on a source vocabulary and apply it to a tokenized\n",
    "  sentence. Note that transforms take list of words and not a sentence.\n",
    "* At line 8, we get the mapping of index to string and then use it get the transformed\n",
    "  sentence\n",
    "\n",
    "Now we will use DataPipe functions to apply transform to all our sentences.\n",
    "Let us define some more functions for this.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([1, 618, 4, 2], [1, 750, 4, 2])\n"
     ]
    }
   ],
   "source": [
    "def applyTransform(sequence_pair):\n",
    "    \"\"\"\n",
    "    Apply transforms to sequence of tokens in a sequence pair\n",
    "    \"\"\"\n",
    "\n",
    "    return (\n",
    "        getTransform(source_vocab)(engTokenize(sequence_pair[0])),\n",
    "        getTransform(target_vocab)(deTokenize(sequence_pair[1]))\n",
    "    )\n",
    "\n",
    "data_pipe = data_pipe.map(applyTransform) ## Apply the function to each element in the iterator\n",
    "temp_list = list(data_pipe)\n",
    "print(temp_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make batches (with bucket batch)\n",
    "Generally, we train models in batches. While working for sequence to sequence models, it is\n",
    "recommended to keep the length of sequences in a batch similar. For that we will use\n",
    "`bucketbatch` function of `data_pipe`.\n",
    "\n",
    "Let us define some functions that will be used by the `bucketbatch` function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sortBucket(bucket):\n",
    "    \"\"\"\n",
    "    Function to sort a given bucket. Here, we want to sort based on the length of\n",
    "    source and target sequence.\n",
    "    \"\"\"\n",
    "    return sorted(bucket, key=lambda x: (len(x[0]), len(x[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will apply the `bucketbatch` function:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_pipe = data_pipe.bucketbatch(\n",
    "    batch_size = 4, batch_num=5,  bucket_num=1,\n",
    "    use_in_batch_shuffle=False, sort_key=sortBucket\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code block:\n",
    "\n",
    "  * We keep batch size = 4.\n",
    "  * `batch_num` is the number of batches to keep in a bucket\n",
    "  * `bucket_num` is the number of buckets to keep in a pool for shuffling\n",
    "  * `sort_key` specifies the function that takes a bucket and sorts it\n",
    "\n",
    "Now, let us consider a batch of source sentences as `X` and a batch of target sentences as `y`.\n",
    "Generally, while training a model, we predict on a batch of `X` and compare the result with `y`.\n",
    "But, a batch in our `data_pipe` is of the form `[(X_1,y_1), (X_2,y_2), (X_3,y_3), (X_4,y_4)]`:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[([1, 5, 435, 4, 2], [1, 7, 669, 414, 4, 2]), ([1, 5, 435, 4, 2], [1, 7, 45, 8136, 4, 2]), ([1, 5, 435, 4, 2], [1, 7, 45, 10516, 4, 2]), ([1, 5, 4889, 4, 2], [1, 7, 45, 3, 4, 2])]\n"
     ]
    }
   ],
   "source": [
    "print(list(data_pipe)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we will now convert them into the form: `((X_1,X_2,X_3,X_4), (y_1,y_2,y_3,y_4))`.\n",
    "For this we will write a small function:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(([1, 6815, 23, 10, 2], [1, 6815, 23, 10, 2], [1, 29, 472, 4, 2], [1, 29, 472, 4, 2]), ([1, 20624, 8, 2], [1, 11009, 8, 2], [1, 31, 1140, 4, 2], [1, 31, 1053, 4, 2]))\n"
     ]
    }
   ],
   "source": [
    "def separateSourceTarget(sequence_pairs):\n",
    "    \"\"\"\n",
    "    input of form: `[(X_1,y_1), (X_2,y_2), (X_3,y_3), (X_4,y_4)]`\n",
    "    output of form: `((X_1,X_2,X_3,X_4), (y_1,y_2,y_3,y_4))`\n",
    "    \"\"\"\n",
    "    sources,targets = zip(*sequence_pairs)\n",
    "    return sources,targets\n",
    "\n",
    "## Apply the function to each element in the iterator\n",
    "data_pipe = data_pipe.map(separateSourceTarget)\n",
    "print(list(data_pipe)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have the data as desired.\n",
    "\n",
    "## Padding\n",
    "We need to pad shorter sentences in a batch to make all the sequences in a batch of equal length. We can perform padding as follows:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def applyPadding(pair_of_sequences):\n",
    "    \"\"\"\n",
    "    Convert sequences to tensors and apply padding\n",
    "    \"\"\"\n",
    "    return (T.ToTensor(0)(list(pair_of_sequences[0])), T.ToTensor(0)(list(pair_of_sequences[1])))\n",
    "## `T.ToTensor(0)` returns a transform that converts the sequence to `torch.tensor` and also applies\n",
    "# padding. Here, `0` is passed to the constructor to specify the index of the `<pad>` token in the\n",
    "# vocabulary.\n",
    "data_pipe = data_pipe.map(applyPadding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can use the index to string mapping to see how the sequence would look with tokens\n",
    "instead of indices:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source:  <sos> Relax . <eos> <pad>\n",
      "Traget:  <sos> Entspann dich . <eos>\n",
      "Source:  <sos> I see . <eos>\n",
      "Traget:  <sos> Aha . <eos> <pad>\n",
      "Source:  <sos> I ran . <eos>\n",
      "Traget:  <sos> Ich rannte . <eos>\n",
      "Source:  <sos> I see . <eos>\n",
      "Traget:  <sos> Ich verstehe . <eos>\n"
     ]
    }
   ],
   "source": [
    "source_index_to_string = source_vocab.get_itos()\n",
    "target_index_to_string = target_vocab.get_itos()\n",
    "\n",
    "def showSomeTransformedSentences(data_pipe):\n",
    "    \"\"\"\n",
    "    Function to show how the sentences look like after applying all transforms.\n",
    "    Here we try to print actual words instead of corresponding index\n",
    "    \"\"\"\n",
    "    for sources,targets in data_pipe:\n",
    "        if sources[0][-1] != 0:\n",
    "            continue # Just to visualize padding of shorter sentences\n",
    "        for i in range(4):\n",
    "            source = \"\"\n",
    "            for token in sources[i]:\n",
    "                source += \" \" + source_index_to_string[token]\n",
    "            target = \"\"\n",
    "            for token in targets[i]:\n",
    "                target += \" \" + target_index_to_string[token]\n",
    "            print(f\"Source: {source}\")\n",
    "            print(f\"Traget: {target}\")\n",
    "        break\n",
    "\n",
    "showSomeTransformedSentences(data_pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above output we can observe that the shorter sentences are padded with `<pad>`. Now, we\n",
    "can use `data_pipe` while writing our training function.\n",
    "\n",
    "Some parts of this tutorial was inspired from [this article](https://medium.com/@bitdribble/migrate-torchtext-to-the-new-0-9-0-api-1ff1472b5d71)_.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
